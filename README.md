# Audio-Visual Speech Enhancement (AVSE) | éŸ³é¢‘è§†è§‰è¯­éŸ³å¢å¼º

## ğŸ“Œ Table of Contents | ç›®å½•
- [English Version](#-english-version) ğŸ‡¬ğŸ‡§
- [ä¸­æ–‡ç‰ˆ](#-ä¸­æ–‡ç‰ˆ) ğŸ‡¨ğŸ‡³

Please review the [LICENSE.md](LICENSE.md) file for details about project usage and permissions.

---

## ğŸ‡¬ğŸ‡§ English Version

Welcome to the **AVSE Project**! ğŸš€ This project explores the use of `Transformer` models for **Audio-Visual Speech Enhancement**. By leveraging **multimodal learning**, we aim to improve speech clarity and enhance user experience. âœ¨

### Project Overview
- **Core Concept**: Utilize **Transformer models** to process **Audio + Visual** information for superior speech enhancement. ğŸ¯
- **Technology Stack**: Deep Learning-based **Multimodal Fusion** for robust audio-visual learning. ğŸ§ 
- **Ultimate Goal**: Improve speech clarity and optimize perceptual experience. ğŸ§

### Mixed Audio
This video showcases the mixed audio, which includes both the target speech and background noise.

![Mixed Audio](Audio_for_github/mixed.mp4)

### Enhanced Audio
This video presents the enhanced audio output, where the background noise has been reduced to emphasize the target speech using the AVSE model.

![Enhanced Audio](Audio_for_github/enhanced.mp4)

---

## ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç‰ˆ

æ¬¢è¿æ¥åˆ° **AVSE é¡¹ç›®**ï¼ğŸš€ æœ¬é¡¹ç›®å°è¯•ä½¿ç”¨ `Transformer` æ¨¡å‹è¿›è¡Œ **éŸ³é¢‘è§†è§‰è¯­éŸ³å¢å¼º**ï¼Œé€šè¿‡ **å¤šæ¨¡æ€å­¦ä¹ ** æé«˜è¯­éŸ³æ¸…æ™°åº¦å’Œç”¨æˆ·ä½“éªŒã€‚âœ¨

### é¡¹ç›®ç®€ä»‹
- **æ ¸å¿ƒç†å¿µ**ï¼šåˆ©ç”¨ **Transformer æ¨¡å‹** å¤„ç† **éŸ³é¢‘ + è§†è§‰** ä¿¡æ¯ï¼Œå®ç°æ›´ä¼˜çš„è¯­éŸ³å¢å¼ºæ•ˆæœ ğŸ¯
- **æŠ€æœ¯æ¡†æ¶**ï¼šåŸºäº **æ·±åº¦å­¦ä¹ ** çš„ **å¤šæ¨¡æ€èåˆ** ğŸ§ 
- **æœ€ç»ˆç›®æ ‡**ï¼šæå‡è¯­éŸ³æ¸…æ™°åº¦ï¼Œä¼˜åŒ–è¯­éŸ³æ„ŸçŸ¥ä½“éªŒ ğŸ§

